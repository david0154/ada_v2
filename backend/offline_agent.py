"""\nDayna AI - Offline Agent\nBilingual (Hindi + English) with Mistral-7B\n\nAuthor: David (Nexuzy Tech)\nLicense: MIT\n"""\n\nimport asyncio\nfrom typing import Optional, Literal, Tuple\nfrom llama_cpp import Llama\nfrom faster_whisper import WhisperModel\nimport subprocess\nimport tempfile\nimport os\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass OfflineAgent:\n    """\n    Offline AI Agent with bilingual support\n    - LLM: Mistral-7B-Instruct GGUF\n    - STT: Whisper (multilingual)\n    - TTS: Piper (Hindi + English)\n    """\n    \n    def __init__(\n        self,\n        model_path: str = "backend/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",\n        whisper_model: str = "base",\n        hindi_voice: str = "backend/models/hi_IN-pratham-medium.onnx",\n        english_voice: str = "backend/models/en_US-lessac-medium.onnx",\n        n_threads: int = 8,\n        n_gpu_layers: int = 35\n    ):\n        print("[DAYNA OFFLINE] üöÄ Initializing...")\n        \n        # Load Mistral-7B\n        print("[DAYNA OFFLINE] Loading Mistral-7B...")\n        try:\n            self.llm = Llama(\n                model_path=model_path,\n                n_ctx=4096,\n                n_threads=n_threads,\n                n_gpu_layers=n_gpu_layers if self._has_gpu() else 0,\n                verbose=False\n            )\n            print("[DAYNA OFFLINE] ‚úÖ Mistral-7B loaded")\n        except Exception as e:\n            print(f"[DAYNA OFFLINE] ‚ùå Failed to load Mistral-7B: {e}")\n            raise\n        \n        # Load Whisper\n        print("[DAYNA OFFLINE] Loading Whisper STT...")\n        try:\n            self.whisper = WhisperModel(\n                whisper_model,\n                device="cuda" if self._has_gpu() else "cpu",\n                compute_type="float16" if self._has_gpu() else "int8"\n            )\n            print("[DAYNA OFFLINE] ‚úÖ Whisper loaded")\n        except Exception as e:\n            print(f"[DAYNA OFFLINE] ‚ùå Failed to load Whisper: {e}")\n            raise\n        \n        # Store TTS paths\n        self.hindi_voice = hindi_voice\n        self.english_voice = english_voice\n        \n        self.conversation_history = []\n        print("[DAYNA OFFLINE] ‚úÖ All systems ready!")\n    \n    def _has_gpu(self) -> bool:\n        """Check if CUDA GPU is available"""\n        try:\n            import torch\n            has_cuda = torch.cuda.is_available()\n            if has_cuda:\n                print(f"[DAYNA OFFLINE] üéÆ GPU detected: {torch.cuda.get_device_name(0)}")\n            else:\n                print("[DAYNA OFFLINE] üíª Using CPU mode")\n            return has_cuda\n        except ImportError:\n            print("[DAYNA OFFLINE] üíª PyTorch not found, using CPU")\n            return False\n    \n    def _detect_language(self, text: str) -> str:\n        """Detect if text is Hindi or English"""\n        if not text:\n            return "en"\n        \n        # Check for Devanagari characters (U+0900 to U+097F)\n        devanagari = sum(1 for c in text if '\u0900' <= c <= '\u097F')\n        total = len([c for c in text if c.isalpha()])\n        \n        if total == 0:\n            return "en"\n        \n        # If more than 30% Devanagari, classify as Hindi\n        return "hi" if (devanagari / total) > 0.3 else "en"\n    \n    async def transcribe_audio(self, audio_path: str) -> Tuple[str, str]:\n        """Transcribe audio and detect language"""\n        segments, info = await asyncio.to_thread(\n            self.whisper.transcribe,\n            audio_path,\n            beam_size=5\n        )\n        \n        text = " ".join([s.text for s in segments]).strip()\n        return text, info.language\n    \n    async def generate_response(\n        self, \n        prompt: str, \n        language: str = "auto",\n        max_tokens: int = 512\n    ) -> Tuple[str, str]:\n        """Generate response in detected language"""\n        \n        if language == "auto":\n            language = self._detect_language(prompt)\n        \n        # Bilingual system prompt\n        if language == "hi":\n            system = "‡§Ü‡§™ Dayna ‡§π‡•à‡§Ç, ‡§è‡§ï AI Assistant‡•§ ‡§Ü‡§™ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§î‡§∞ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§§ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ User ‡§ú‡§ø‡§∏ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§õ‡•á, ‡§â‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§Ç‡•§"\n        else:\n            system = "You are Dayna, an AI Assistant. You can speak both Hindi and English. Respond in the same language as the user."\n        \n        # Format prompt for Mistral-Instruct\n        formatted = f"<s>[INST] {system}\n\n{prompt} [/INST]"\n        \n        # Generate response\n        response = await asyncio.to_thread(\n            self.llm,\n            formatted,\n            max_tokens=max_tokens,\n            temperature=0.7,\n            top_p=0.95,\n            stop=["</s>", "[INST]"],\n            echo=False\n        )\n        \n        text = response['choices'][0]['text'].strip()\n        detected_lang = self._detect_language(text)\n        \n        # Update conversation history\n        self.conversation_history.append({\n            "role": "user",\n            "content": prompt,\n            "language": language\n        })\n        self.conversation_history.append({\n            "role": "assistant",\n            "content": text,\n            "language": detected_lang\n        })\n        \n        # Keep only last 10 exchanges\n        if len(self.conversation_history) > 20:\n            self.conversation_history = self.conversation_history[-20:]\n        \n        return text, detected_lang\n    \n    async def synthesize_speech(\n        self,\n        text: str,\n        language: str = "auto"\n    ) -> bytes:\n        """Generate TTS audio"""\n        \n        if language == "auto":\n            language = self._detect_language(text)\n        \n        voice = self.hindi_voice if language == "hi" else self.english_voice\n        voice_name = "Hindi (Pratham)" if language == "hi" else "English (Lessac)"\n        \n        print(f"[DAYNA TTS] üó£Ô∏è Using {voice_name}")\n        \n        # Create temp output file\n        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:\n            output_path = f.name\n        \n        try:\n            # Run Piper TTS\n            process = await asyncio.create_subprocess_exec(\n                "piper",\n                "--model", voice,\n                "--output_file", output_path,\n                stdin=asyncio.subprocess.PIPE,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE\n            )\n            \n            stdout, stderr = await process.communicate(input=text.encode('utf-8'))\n            \n            if process.returncode != 0:\n                error_msg = stderr.decode('utf-8')\n                raise Exception(f"Piper TTS failed: {error_msg}")\n            \n            # Read generated audio\n            with open(output_path, 'rb') as f:\n                audio = f.read()\n            \n            return audio\n            \n        finally:\n            # Clean up temp file\n            if os.path.exists(output_path):\n                os.unlink(output_path)\n    \n    async def process_voice_input(\n        self,\n        audio_path: str,\n        force_language: Optional[str] = None\n    ) -> Tuple[str, bytes, str]:\n        """\n        Complete offline voice pipeline:\n        1. Transcribe speech (auto-detect language)\n        2. Generate LLM response\n        3. Synthesize TTS audio\n        \n        Returns: (response_text, audio_bytes, language_used)\n        """\n        print(f"[DAYNA VOICE] üé§ Processing: {audio_path}")\n        \n        # Step 1: Speech to Text\n        user_text, detected_lang = await self.transcribe_audio(audio_path)\n        lang_name = "Hindi" if detected_lang == "hi" else "English"\n        print(f"[DAYNA USER] ({lang_name}): {user_text}")\n        \n        # Step 2: Generate Response\n        response_text, response_lang = await self.generate_response(\n            user_text,\n            language=force_language or detected_lang\n        )\n        resp_lang_name = "Hindi" if response_lang == "hi" else "English"\n        print(f"[DAYNA AI] ({resp_lang_name}): {response_text}")\n        \n        # Step 3: Text to Speech\n        audio_bytes = await self.synthesize_speech(response_text, response_lang)\n        \n        print(f"[DAYNA VOICE] ‚úÖ Complete")\n        return response_text, audio_bytes, response_lang\n    \n    def reset_conversation(self):\n        """Clear conversation history"""\n        self.conversation_history = []\n        print("[DAYNA] üóëÔ∏è Conversation history cleared")\n\n\nif __name__ == "__main__":\n    async def test():\n        print("\n" + "="*50)\n        print("Dayna AI - Offline Agent Test")\n        print("="*50 + "\n")\n        \n        agent = OfflineAgent()\n        \n        # Test English\n        print("\n--- Testing English ---")\n        resp_en, lang = await agent.generate_response("Hello! What is your name?")\n        print(f"Response ({lang}): {resp_en}")\n        \n        # Test Hindi\n        print("\n--- Testing Hindi ---")\n        resp_hi, lang = await agent.generate_response("‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?")\n        print(f"Response ({lang}): {resp_hi}")\n        \n    asyncio.run(test())\n